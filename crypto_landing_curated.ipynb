{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a65541-db17-4f6e-b2f3-e481dd7bda09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # dbutils.widgets.text(\"file_tm\", \"\")\n",
    "\n",
    "# # file_name = dbutils.widgets.get(\"file_tm\")\n",
    "# # input_path = f\"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/landing/{file_name}\"\n",
    "\n",
    "# # df = spark.read.option(\"multiline\", True).json(input_path)\n",
    "\n",
    "# # output_path = \"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/curated\"\n",
    "# # # Merge or Append logic here\n",
    "# # df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(output_path)\n",
    "\n",
    "\n",
    "\n",
    "# # ---------------------------------------------\n",
    "# # 1. Get filename passed from ADF pipeline\n",
    "# # ---------------------------------------------\n",
    "# dbutils.widgets.text(\"file_tm\", \"\")\n",
    "# file_name = dbutils.widgets.get(\"file_tm\")\n",
    "\n",
    "# # Input file path\n",
    "# input_path = f\"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/landing/{file_name}\"\n",
    "\n",
    "# # Output folder path\n",
    "# output_path = \"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/curated\"\n",
    "\n",
    "# # ---------------------------------------------\n",
    "# # 2. Read the JSON file\n",
    "# # ---------------------------------------------\n",
    "# try:\n",
    "#     df = spark.read.option(\"multiline\", True).json(input_path)\n",
    "\n",
    "#     if df.isEmpty():\n",
    "#         print(f\"‚ùå No data found in file: {file_name}\")\n",
    "#     else:\n",
    "#         # ---------------------------------------------\n",
    "#         # 3. Append to Delta in curated zone\n",
    "#         # ---------------------------------------------\n",
    "#         df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(output_path)\n",
    "#         print(f\"‚úÖ Data written to curated successfully from file: {file_name}\")\n",
    "\n",
    "#         # ---------------------------------------------\n",
    "#         # 4. Delete the processed input file\n",
    "#         # ---------------------------------------------\n",
    "#         dbutils.fs.rm(input_path, recurse=True)\n",
    "#         print(f\"üóëÔ∏è Deleted file from landing: {input_path}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå ERROR processing file: {file_name}\")\n",
    "#     raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e532a2a-455e-4830-9e42-91df3fc420b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, from_unixtime\n",
    "# from delta.tables import DeltaTable\n",
    "\n",
    "# # ---------------------------------------------\n",
    "# # 1. Get the input file name from ADF\n",
    "# # ---------------------------------------------\n",
    "# dbutils.widgets.text(\"file_tm\", \"\")\n",
    "# file_name = dbutils.widgets.get(\"file_tm\")\n",
    "\n",
    "# # Define input and output paths\n",
    "# input_path = f\"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/landing/{file_name}\"\n",
    "# output_path = \"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/curated/bitcoin_data\"\n",
    "\n",
    "# # ---------------------------------------------\n",
    "# # 2. Read the JSON file\n",
    "# # ---------------------------------------------\n",
    "# try:\n",
    "#     raw_df = spark.read.option(\"multiline\", True).json(input_path)\n",
    "\n",
    "#     if raw_df.isEmpty():\n",
    "#         print(f\"No data found in file: {file_name}\")\n",
    "#     else:\n",
    "#         # ---------------------------------------------\n",
    "#         # 3. Extract and transform required fields\n",
    "#         # ---------------------------------------------\n",
    "#         df = raw_df.selectExpr(\"explode(data) as record\") \\\n",
    "#                    .select(\n",
    "#                        col(\"record.priceUsd\").cast(\"double\").alias(\"priceUsd\"),\n",
    "#                        col(\"record.time\").cast(\"long\").alias(\"time_unix\"),\n",
    "#                        from_unixtime(col(\"record.time\") / 1000).cast(\"timestamp\").alias(\"timestamp\"),\n",
    "#                        col(\"record.circulatingSupply\").cast(\"double\").alias(\"circulatingSupply\")\n",
    "#                    )\n",
    "\n",
    "#         # ---------------------------------------------\n",
    "#         # 4. Create Delta table if not exists\n",
    "#         # ---------------------------------------------\n",
    "#         if not spark._jsparkSession.catalog().tableExists(\"curated_bitcoin_data\"):\n",
    "#             df.write.format(\"delta\").save(output_path)\n",
    "#             spark.sql(f\"CREATE TABLE curated_bitcoin_data USING DELTA LOCATION '{output_path}'\")\n",
    "#             print(f\"Delta table created at: {output_path}\")\n",
    "\n",
    "#         # ---------------------------------------------\n",
    "#         # 5. Merge into Delta table to prevent duplicates\n",
    "#         # ---------------------------------------------\n",
    "#         delta_table = DeltaTable.forPath(spark, output_path)\n",
    "\n",
    "#         delta_table.alias(\"target\").merge(\n",
    "#             df.alias(\"source\"),\n",
    "#             \"target.timestamp = source.timestamp\"\n",
    "#         ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "#         print(f\"Data merged to curated zone successfully from file: {file_name}\")\n",
    "\n",
    "#         # ---------------------------------------------\n",
    "#         # 6. Delete the processed input file\n",
    "#         # ---------------------------------------------\n",
    "#         dbutils.fs.rm(input_path, recurse=True)\n",
    "#         print(f\"Deleted file from landing: {input_path}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"ERROR processing file: {file_name}\")\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b6ec5a-452b-4644-add3-96e961644e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %python\n",
    "# from pyspark.sql.functions import col, from_unixtime, to_timestamp\n",
    "# from delta.tables import DeltaTable\n",
    "\n",
    "# # ---------------------------------------------\n",
    "# # 1. Get the input file name from ADF\n",
    "# # ---------------------------------------------\n",
    "# dbutils.widgets.text(\"file_tm\", \"\")\n",
    "# file_name = dbutils.widgets.get(\"file_tm\")\n",
    "\n",
    "# # Define input and output paths\n",
    "# input_path = f\"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/landing/{file_name}\"\n",
    "# output_path = \"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/curated/\"\n",
    "\n",
    "# # ---------------------------------------------\n",
    "# # 2. Read the JSON file\n",
    "# # ---------------------------------------------\n",
    "# try:\n",
    "#     raw_df = spark.read.option(\"multiline\", True).json(input_path)\n",
    "\n",
    "#     if raw_df.isEmpty():\n",
    "#         print(f\"No data found in file: {file_name}\")\n",
    "#     else:\n",
    "#         # ---------------------------------------------\n",
    "#         # 3. Extract and transform required fields\n",
    "#         # ---------------------------------------------\n",
    "#         # df = raw_df.select\n",
    "#             col(\"id\"),\n",
    "#             col(\"symbol\"),\n",
    "#             col(\"name\"),\n",
    "#             col(\"current_price\").cast(\"double\").alias(\"price_usd\"),\n",
    "#             col(\"circulating_supply\").cast(\"double\"),\n",
    "#             col(\"market_cap\").cast(\"double\"),\n",
    "#             col(\"total_volume\").cast(\"double\"),\n",
    "#             col(\"last_updated\").alias(\"timestamp\")\n",
    "#         ).withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "#         # ---------------------------------------------\n",
    "#         # 4. Create Delta table if not exists\n",
    "#         # ---------------------------------------------\n",
    "#         if not spark.catalog.tableExists(\"curated_bitcoin_data\"):\n",
    "#             df.write.format(\"delta\").save(output_path)\n",
    "#             spark.sql(f\"CREATE TABLE curated_bitcoin_data USING DELTA LOCATION '{output_path}'\")\n",
    "#             print(f\"Delta table created at: {output_path}\")\n",
    "\n",
    "#         # ---------------------------------------------\n",
    "#         # 5. Merge into Delta table to prevent duplicates\n",
    "#         # ---------------------------------------------\n",
    "#         delta_table = DeltaTable.forPath(spark, output_path)\n",
    "\n",
    "#         delta_table.alias(\"target\").merge(\n",
    "#             df.alias(\"source\"),\n",
    "#             \"target.timestamp = source.timestamp\"\n",
    "#         ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "#         print(f\"Data merged to curated zone successfully from file: {file_name}\")\n",
    "\n",
    "#         # ---------------------------------------------\n",
    "#         # 6. Delete the processed input file\n",
    "#         # ---------------------------------------------\n",
    "#         dbutils.fs.rm(input_path, recurse=True)\n",
    "#         print(f\"Deleted file from landing: {input_path}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"ERROR processing file: {file_name}\")\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdca3069-6845-425f-a711-6007e50bc1b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %python\n",
    "# from pyspark.sql.functions import col, to_timestamp, to_date\n",
    "# from delta.tables import DeltaTable\n",
    "\n",
    "# # ---------------------------------------------\n",
    "# # 1. Get the input file name from ADF\n",
    "# # ---------------------------------------------\n",
    "# dbutils.widgets.text(\"file_tm\", \"\")\n",
    "# file_name = dbutils.widgets.get(\"file_tm\")\n",
    "\n",
    "# # Define input and output paths\n",
    "# input_path = f\"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/landing/{file_name}\"\n",
    "# output_path = \"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/curated/\"\n",
    "\n",
    "# # ---------------------------------------------\n",
    "# # 2. Read the JSON file\n",
    "# # ---------------------------------------------\n",
    "# try:\n",
    "#     raw_df = spark.read.option(\"multiline\", True).json(input_path)\n",
    "\n",
    "#     if raw_df.isEmpty():\n",
    "#         print(f\"No data found in file: {file_name}\")\n",
    "#     else:\n",
    "#         # ---------------------------------------------\n",
    "#         # 3. Extract and transform required fields\n",
    "#         # ---------------------------------------------\n",
    "#         df = raw_df.select(\n",
    "#             col(\"id\"),\n",
    "#             col(\"symbol\"),\n",
    "#             col(\"name\"),\n",
    "#             col(\"current_price\").cast(\"double\").alias(\"price_usd\"),\n",
    "#             col(\"circulating_supply\").cast(\"double\"),\n",
    "#             col(\"market_cap\").cast(\"double\"),\n",
    "#             col(\"total_volume\").cast(\"double\"),\n",
    "#             col(\"last_updated\").alias(\"timestamp\")\n",
    "#         ).withColumn(\"timestamp\", to_timestamp(\"timestamp\")) \\\n",
    "#          .withColumn(\"date\", to_date(\"timestamp\"))\n",
    "\n",
    "#         # ---------------------------------------------\n",
    "#         # 4. Create Delta table if not exists\n",
    "#         # ---------------------------------------------\n",
    "#         if not DeltaTable.isDeltaTable(spark, output_path):\n",
    "#             df.write.format(\"delta\") \\\n",
    "#                 .mode(\"overwrite\") \\\n",
    "#                 .option(\"mergeSchema\", \"true\") \\\n",
    "#                 .partitionBy(\"id\", \"date\") \\\n",
    "#                 .save(output_path)\n",
    "            \n",
    "#             spark.sql(f\"\"\"\n",
    "#                 CREATE TABLE IF NOT EXISTS curated_bitcoin_data \n",
    "#                 USING DELTA \n",
    "#                 LOCATION '{output_path}'\n",
    "#             \"\"\")\n",
    "#             print(f\"Delta table created at: {output_path} (partitioned by id, date)\")\n",
    "\n",
    "#         # ---------------------------------------------\n",
    "#         # 5. Merge into Delta table to prevent duplicates\n",
    "#         # ---------------------------------------------\n",
    "#         delta_table = DeltaTable.forPath(spark, output_path)\n",
    "\n",
    "#         delta_table.alias(\"target\").merge(\n",
    "#             df.alias(\"source\"),\n",
    "#             \"target.timestamp = source.timestamp AND target.symbol = source.symbol\"\n",
    "#         ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "#         print(f\"Data merged to curated zone successfully from file: {file_name}\")\n",
    "\n",
    "#         # ---------------------------------------------\n",
    "#         # 6. Delete the processed input file\n",
    "#         # ---------------------------------------------\n",
    "#         dbutils.fs.rm(input_path, recurse=True)\n",
    "#         print(f\"Deleted file from landing: {input_path}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"ERROR processing file: {file_name}\")\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "555b6e84-81d7-4e4a-842e-ba1f8d95c1bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date, lit, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Get the input file name from ADF\n",
    "# ---------------------------------------------\n",
    "dbutils.widgets.text(\"file_tm\", \"\")\n",
    "file_name = dbutils.widgets.get(\"file_tm\")\n",
    "\n",
    "# Define input and output paths\n",
    "input_path = f\"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/landing/{file_name}\"\n",
    "output_path = \"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/curated/\"\n",
    "audit_path = \"abfss://cryptoreporting@cryptoreportingadlssa.dfs.core.windows.net/audit/logs\"\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Read the JSON file\n",
    "# ---------------------------------------------\n",
    "try:\n",
    "    raw_df = spark.read.option(\"multiline\", True).json(input_path)\n",
    "\n",
    "    if raw_df.isEmpty():\n",
    "        print(f\"No data found in file: {file_name}\")\n",
    "    else:\n",
    "        # ---------------------------------------------\n",
    "        # 3. Extract and transform required fields\n",
    "        # ---------------------------------------------\n",
    "        df = raw_df.select(\n",
    "            col(\"id\"),\n",
    "            col(\"symbol\"),\n",
    "            col(\"name\"),\n",
    "            col(\"current_price\").cast(\"double\").alias(\"price_usd\"),\n",
    "            col(\"circulating_supply\").cast(\"double\"),\n",
    "            col(\"market_cap\").cast(\"double\"),\n",
    "            col(\"total_volume\").cast(\"double\"),\n",
    "            col(\"last_updated\").alias(\"timestamp\")\n",
    "        ).withColumn(\"timestamp\", to_timestamp(\"timestamp\")) \\\n",
    "         .withColumn(\"date\", to_date(\"timestamp\")) \\\n",
    "         .withColumn(\"ingestion_time\", current_timestamp()) \\\n",
    "         .withColumn(\"source_file\", lit(file_name))\n",
    "\n",
    "        record_count = df.count()\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 4. Create Delta table if not exists\n",
    "        # ---------------------------------------------\n",
    "        if not DeltaTable.isDeltaTable(spark, output_path):\n",
    "            df.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .partitionBy(\"id\", \"date\") \\\n",
    "                .save(output_path)\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS curated_bitcoin_data \n",
    "                USING DELTA \n",
    "                LOCATION '{output_path}'\n",
    "            \"\"\")\n",
    "            print(f\"Delta table created at: {output_path} (partitioned by id, date)\")\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 5. Merge into Delta table to prevent duplicates\n",
    "        # ---------------------------------------------\n",
    "        delta_table = DeltaTable.forPath(spark, output_path)\n",
    "\n",
    "        delta_table.alias(\"target\").merge(\n",
    "            df.alias(\"source\"),\n",
    "            \"target.timestamp = source.timestamp AND target.symbol = source.symbol\"\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "        print(f\"{record_count} records merged successfully from file: {file_name}\")\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 6. Delete the processed input file\n",
    "        # ---------------------------------------------\n",
    "        dbutils.fs.rm(input_path, recurse=True)\n",
    "        print(f\"Deleted input file: {input_path}\")\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 7. Log audit details\n",
    "        # ---------------------------------------------\n",
    "        audit_df = spark.createDataFrame([{\n",
    "            \"file_name\": file_name,\n",
    "            \"record_count\": record_count,\n",
    "            \"status\": \"Success\",\n",
    "            \"processed_at\": str(current_timestamp())\n",
    "        }])\n",
    "        audit_df.write.format(\"delta\").mode(\"append\").save(audit_path)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 8. Optimize and Vacuum curated table (optional)\n",
    "        # ---------------------------------------------\n",
    "        spark.sql(\"OPTIMIZE delta.`{}`\".format(output_path))\n",
    "        spark.sql(\"VACUUM delta.`{}` RETAIN 168 HOURS\".format(output_path))  # Retains last 7 days\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR processing file: {file_name}\")\n",
    "    error_df = spark.createDataFrame([{\n",
    "        \"file_name\": file_name,\n",
    "        \"record_count\": 0,\n",
    "        \"status\": f\"Failed: {str(e)}\",\n",
    "        \"processed_at\": str(current_timestamp())\n",
    "    }])\n",
    "    error_df.write.format(\"delta\").mode(\"append\").save(audit_path)\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "crypto_landing_curated",
   "widgets": {
    "file_tm": {
     "currentValue": "",
     "nuid": "6a5d2326-cc0c-4df8-993b-22e948b05547",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "file_tm",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "file_tm",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
